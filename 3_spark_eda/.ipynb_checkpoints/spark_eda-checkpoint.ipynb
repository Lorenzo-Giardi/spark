{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('Spark EDA') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/lorenzo/spark-repo/1_spark_dataframes/data/california.csv'\n",
    "\n",
    "df1 = spark.read.option('header', 'True') \\\n",
    "                .option('inferSchema', 'True') \\\n",
    "                .csv(data_path)\n",
    "\n",
    "df1 = df1.withColumnRenamed('latitude', 'lat') \\\n",
    "         .withColumnRenamed('longitude', 'lng') \\\n",
    "         .withColumnRenamed('total_rooms', 'rooms') \\\n",
    "         .withColumnRenamed('total_bedrooms', 'bedrooms') \\\n",
    "         .withColumnRenamed('median_income', 'income') \\\n",
    "         .withColumnRenamed('median_house_value', 'value') \\\n",
    "         .withColumnRenamed('housing_median_age', 'age') \n",
    "\n",
    "df1.createOrReplaceTempView('california')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics\n",
    "\n",
    "**Describe() method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|             rooms|            income|               age|             value|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|             20640|             20640|             20640|             20640|\n",
      "|   mean|2635.7630813953488|3.8706710029070246|28.639486434108527|206855.81690891474|\n",
      "| stddev|2181.6152515827944| 1.899821717945263| 12.58555761211163|115395.61587441359|\n",
      "|    min|               2.0|            0.4999|               1.0|           14999.0|\n",
      "|    max|           39320.0|           15.0001|              52.0|          500001.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.select('rooms', 'income', 'age', 'value').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13415311380656275"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.stat.corr('rooms', 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('rooms_per_person', df1.rooms/df1.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.209481969006692"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.stat.corr('rooms_per_person', 'value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequent values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       age_freqItems|\n",
      "+--------------------+\n",
      "|[23.0, 32.0, 41.0...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.stat.freqItems(['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       age_freqItems|     value_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[23.0, 32.0, 41.0...|[55000.0, 68700.0...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.stat.freqItems(['age', 'value']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crosstable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|ocean_proximity_age|1.0|10.0|2.0|3.0|4.0|5.0|6.0|7.0|8.0|9.0|\n",
      "+-------------------+---+----+---+---+---+---+---+---+---+---+\n",
      "|             INLAND|  4| 151| 33| 33| 77|122| 80|104|112|108|\n",
      "|           NEAR BAY|  0|  19|  4|  4|  9| 17|  7|  4|  7|  4|\n",
      "|         NEAR OCEAN|  0|  16|  2|  2| 26| 20| 11| 18| 18| 23|\n",
      "|          <1H OCEAN|  0|  78| 19| 23| 79| 85| 62| 49| 69| 70|\n",
      "+-------------------+---+----+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1.age <= 10).stat.crosstab('ocean_proximity', 'age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bucketing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|count(1)|age_bucket|\n",
      "+--------+----------+\n",
      "|       4|        10|\n",
      "|      58|        20|\n",
      "|      62|        30|\n",
      "|     191|        40|\n",
      "|     244|        50|\n",
      "|     160|        60|\n",
      "|     175|        70|\n",
      "|     206|        80|\n",
      "|     205|        90|\n",
      "|     264|       100|\n",
      "|     254|       110|\n",
      "|     238|       120|\n",
      "|     302|       130|\n",
      "|     412|       140|\n",
      "|     512|       150|\n",
      "|     771|       160|\n",
      "|     698|       170|\n",
      "|     570|       180|\n",
      "|     502|       190|\n",
      "|     465|       200|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*), FLOOR(age*10) as age_bucket \\\n",
    "           FROM california GROUP BY age_bucket ORDER BY age_bucket\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|count(1)|round(avg(value), 2)|age_bucket|\n",
      "+--------+--------------------+----------+\n",
      "|       4|            144300.0|        10|\n",
      "|      58|           224475.91|        20|\n",
      "|      62|           235643.58|        30|\n",
      "|     191|           229235.14|        40|\n",
      "|     244|           208417.66|        50|\n",
      "|     160|           203794.39|        60|\n",
      "|     175|           193296.03|        70|\n",
      "|     206|           194414.58|        80|\n",
      "|     205|            186672.7|        90|\n",
      "|     264|            176580.7|       100|\n",
      "|     254|            179907.9|       110|\n",
      "|     238|           182046.24|       120|\n",
      "|     302|           190181.81|       130|\n",
      "|     412|            189597.1|       140|\n",
      "|     512|           181808.42|       150|\n",
      "|     771|           200180.56|       160|\n",
      "|     698|           190494.29|       170|\n",
      "|     570|           193354.77|       180|\n",
      "|     502|           193197.84|       190|\n",
      "|     465|           195659.82|       200|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*), round(mean(value), 2), FLOOR(age*10) as age_bucket \\\n",
    "           FROM california GROUP BY age_bucket ORDER BY age_bucket\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/lorenzo/Desktop/utilization.csv'\n",
    "\n",
    "df2 = spark.read.option('header', 'False') \\\n",
    "                .option('inferSchema', 'True') \\\n",
    "                .csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumnRenamed(\"_c0\", \"event_datetime\") \\\n",
    "            .withColumnRenamed (\"_c1\", \"server_id\") \\\n",
    "            .withColumnRenamed(\"_c2\", \"cpu_utilization\") \\\n",
    "            .withColumnRenamed(\"_c3\", \"free_memory\") \\\n",
    "            .withColumnRenamed(\"_c4\", \"session_count\")\n",
    "\n",
    "df2.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+\n",
      "|      148|                0.54|                0.94|\n",
      "|      137|                0.54|                0.94|\n",
      "|      133|                0.55|                0.95|\n",
      "|      108|                0.55|                0.95|\n",
      "|      101|                 0.6|                 1.0|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT server_id, min(cpu_utilization), max(cpu_utilization) \\\n",
    "           FROM utilization \\\n",
    "           GROUP BY server_id\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Windowing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|      avg_cpu_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|0.7393840000000045|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.7393840000000045|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7393840000000045|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7393840000000045|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 08:37:41|      148|           0.77|0.7393840000000045|\n",
      "|03/05/2019 08:42:41|      148|           0.59|0.7393840000000045|\n",
      "|03/05/2019 08:47:41|      148|           0.77|0.7393840000000045|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7393840000000045|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                  avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_cpu_util \\\n",
    "           FROM utilization\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|      avg_cpu_util|      delta_cpu_util|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|0.7393840000000045|  0.1106159999999955|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.7393840000000045| 0.20061599999999546|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7393840000000045| 6.15999999995509E-4|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7393840000000045|-0.10938400000000448|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 08:37:41|      148|           0.77|0.7393840000000045|0.030615999999995536|\n",
      "|03/05/2019 08:42:41|      148|           0.59|0.7393840000000045| -0.1493840000000045|\n",
      "|03/05/2019 08:47:41|      148|           0.77|0.7393840000000045|0.030615999999995536|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7393840000000045|-0.02938400000000...|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                  avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_cpu_util, \\\n",
    "                  cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) as delta_cpu_util\\\n",
    "           FROM utilization\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moving window:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|      avg_cpu_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|             0.895|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.8933333333333334|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.8566666666666668|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7533333333333333|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7533333333333334|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7633333333333333|\n",
      "|03/05/2019 08:37:41|      148|           0.77|              0.75|\n",
      "|03/05/2019 08:42:41|      148|           0.59|              0.71|\n",
      "|03/05/2019 08:47:41|      148|           0.77|              0.69|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7766666666666667|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime, server_id, cpu_utilization, \\\n",
    "                  avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime  \\\n",
    "                        ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) as avg_cpu_util \\\n",
    "           FROM utilization\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpy36",
   "language": "python",
   "name": "newpy36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
