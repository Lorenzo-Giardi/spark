{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-ml Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StandardScaler, Bucketizer, \\\n",
    "                                Tokenizer, HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  1|[10.0,10000.0,1.0]|\n",
      "|  2|[15.0,30000.0,2.0]|\n",
      "|  3|[30.0,40000.0,3.0]|\n",
      "|  4|[20.0,35000.0,1.0]|\n",
      "|  5|[40.0,20000.0,2.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df = spark.createDataFrame([\n",
    "            (1, Vectors.dense([10.0, 10000.0, 1.0]),),\n",
    "            (2, Vectors.dense([15.0, 30000.0, 2.0]),),\n",
    "            (3, Vectors.dense([30.0, 40000.0, 3.0]),),\n",
    "            (4, Vectors.dense([20.0, 35000.0, 1.0]),),\n",
    "            (5, Vectors.dense([40.0, 20000.0, 2.0]),),\n",
    "    ], ['id', 'features'])\n",
    "\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max-Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmx_scaler = MinMaxScaler(inputCol='features', outputCol='sfeatures')\n",
    "mmx_model = mmx_scaler.fit(features_df)\n",
    "mmx_output = mmx_model.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([40.0, 40000.0, 3.0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmx_model.originalMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([10.0, 10000.0, 1.0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmx_model.originalMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+\n",
      "| id|          features|           sfeatures|\n",
      "+---+------------------+--------------------+\n",
      "|  1|[10.0,10000.0,1.0]|       [0.0,0.0,0.0]|\n",
      "|  2|[15.0,30000.0,2.0]|[0.16666666666666...|\n",
      "|  3|[30.0,40000.0,3.0]|[0.66666666666666...|\n",
      "|  4|[20.0,35000.0,1.0]|[0.33333333333333...|\n",
      "|  5|[40.0,20000.0,2.0]|[1.0,0.3333333333...|\n",
      "+---+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mmx_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler(inputCol='features', outputCol='sfeatures', withMean=True, withStd=True)\n",
    "std_model = std_scaler.fit(features_df)\n",
    "std_output = std_model.transform(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([23.0, 27000.0, 1.8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_model.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([12.0416, 12041.5946, 0.8367])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_model.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+\n",
      "| id|          features|           sfeatures|\n",
      "+---+------------------+--------------------+\n",
      "|  1|[10.0,10000.0,1.0]|[-1.0795912380986...|\n",
      "|  2|[15.0,30000.0,2.0]|[-0.6643638388299...|\n",
      "|  3|[30.0,40000.0,3.0]|[0.58131835897617...|\n",
      "|  4|[20.0,35000.0,1.0]|[-0.2491364395612...|\n",
      "|  5|[40.0,20000.0,2.0]|[1.41177315751357...|\n",
      "+---+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "std_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|features|\n",
      "+--------+\n",
      "|  -435.0|\n",
      "|   -53.0|\n",
      "|    -8.0|\n",
      "|     2.0|\n",
      "|     6.0|\n",
      "|    27.0|\n",
      "|   368.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define threshold for buckets\n",
    "splits = [-float(\"inf\"), -10, 0.0, 10.0, float(\"inf\")]\n",
    "\n",
    "# define unidimensional dataframe\n",
    "b_data = [(-435.0,),(-53.0,),(-8.0,),(2.0,),(6.0,),(27.0,),(368.0,)]\n",
    "b_df = spark.createDataFrame(b_data, [\"features\"])\n",
    "b_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bck = Bucketizer(inputCol='features', outputCol='bfeatures', splits=splits)\n",
    "bck_output = bck.transform(b_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|features|bfeatures|\n",
      "+--------+---------+\n",
      "|  -435.0|      0.0|\n",
      "|   -53.0|      0.0|\n",
      "|    -8.0|      1.0|\n",
      "|     2.0|      2.0|\n",
      "|     6.0|      2.0|\n",
      "|    27.0|      3.0|\n",
      "|   368.0|      3.0|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bck_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Converts the input string to lowercase and then splits it by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  1|This is an introd...|\n",
      "|  2|The cat is on the...|\n",
      "|  3|The dog is under ...|\n",
      "|  4|The quick brown f...|\n",
      "|  5|Hello, this is a ...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences_df = spark.createDataFrame([\n",
    "            (1, \"This is an introduction to Spark ML\"),\n",
    "            (2, \"The cat is on the table\"),\n",
    "            (3, \"The dog is under the table\"),\n",
    "            (4, \"The quick brown fox jumps over the lazy dog\"),\n",
    "            (5, \"Hello, this is a spark dataframe\"),\n",
    "    ], ['id', 'sentence'])\n",
    "\n",
    "sentences_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "tokenizer_out = token.transform(sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|\n",
      "|  2|The cat is on the...|[the, cat, is, on...|\n",
      "|  3|The dog is under ...|[the, dog, is, un...|\n",
      "|  4|The quick brown f...|[the, quick, brow...|\n",
      "|  5|Hello, this is a ...|[hello,, this, is...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_out.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|               words|         rawFeatures|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|(32,[1,4,8,17,20,...|\n",
      "|  2|The cat is on the...|[the, cat, is, on...|(32,[2,16,17,29,3...|\n",
      "|  3|The dog is under ...|[the, dog, is, un...|(32,[15,17,29,30,...|\n",
      "|  4|The quick brown f...|[the, quick, brow...|(32,[0,5,15,16,19...|\n",
      "|  5|Hello, this is a ...|[hello,, this, is...|(32,[1,17,18,25,2...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=32)\n",
    "hashTF_df = hashingTF.transform(tokenizer_out)\n",
    "hashTF_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|            sentence|               words|         rawFeatures|        idf_features|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  1|This is an introd...|[this, is, an, in...|(32,[1,4,8,17,20,...|(32,[1,4,8,17,20,...|\n",
      "|  2|The cat is on the...|[the, cat, is, on...|(32,[2,16,17,29,3...|(32,[2,16,17,29,3...|\n",
      "|  3|The dog is under ...|[the, dog, is, un...|(32,[15,17,29,30,...|(32,[15,17,29,30,...|\n",
      "|  4|The quick brown f...|[the, quick, brow...|(32,[0,5,15,16,19...|(32,[0,5,15,16,19...|\n",
      "|  5|Hello, this is a ...|[hello,, this, is...|(32,[1,17,18,25,2...|(32,[1,17,18,25,2...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"idf_features\")\n",
    "idf_model = idf.fit(hashTF_df)\n",
    "idf_out = idf_model.transform(hashTF_df)\n",
    "idf_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newpy36",
   "language": "python",
   "name": "newpy36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
